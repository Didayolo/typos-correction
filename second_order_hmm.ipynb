{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle de Markov Caché du second ordre\n",
    "\n",
    "### Application à la correction de typos dans des textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from numpy import array, ones, zeros, multiply\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "UNK = \"<unk>\"  # token to map all out-of-vocabulary words (OOVs)\n",
    "UNKid = 0      # index for UNK\n",
    "epsilon=1e-100\n",
    "\n",
    "class HMM:\n",
    "        def __init__(self, state_list, observation_list,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None,\n",
    "                  transition_head_proba=None, smoothing_obs = 0.01):\n",
    "            \"\"\"\n",
    "            Builds a Hidden Markov Model\n",
    "            * state_list is the list of state symbols [q_0...q_(N-1)]\n",
    "            * observation_list is the list of observation symbols [v_0...v_(M-1)]\n",
    "            * transition_proba is the transition probability matrix\n",
    "                [a_ij] a_ij = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "            * observation_proba is the observation probablility matrix\n",
    "                [b_ki] b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "            * initial_state_proba is the initial state distribution\n",
    "                [pi_i] pi_i = Pr(Y_0=q_i)\"\"\"\n",
    "            \n",
    "            print \"HMM creating with: \"\n",
    "            \n",
    "            self.N = len(state_list)       # number of states\n",
    "            self.M = len(observation_list) # number of possible emissions\n",
    "            \n",
    "            print str(self.N)+\" states\"\n",
    "            print str(self.M)+\" observations\"\n",
    "            \n",
    "            self.omega_Y = state_list\n",
    "            self.omega_X = observation_list\n",
    "            \n",
    "            if transition_proba is None:\n",
    "                self.transition_proba = zeros( (self.N, self.N), float) \n",
    "            else:\n",
    "                self.transition_proba=transition_proba\n",
    "            \n",
    "            if observation_proba is None:\n",
    "                self.observation_proba = zeros( (self.M, self.N), float) \n",
    "            else:\n",
    "                self.observation_proba=observation_proba\n",
    "            \n",
    "            if initial_state_proba is None:\n",
    "                self.initial_state_proba = zeros( (self.N,), float ) \n",
    "            else:\n",
    "                self.initial_state_proba=initial_state_proba\n",
    "            \n",
    "            if transition_head_proba is None:\n",
    "                self.transition_head_proba = zeros((self.N, self.N), float)\n",
    "            else:\n",
    "                self.transition_head_proba = transition_head_proba\n",
    "            \n",
    "            self.make_indexes() # build indexes, i.e the mapping between token and int\n",
    "            self.smoothing_obs = smoothing_obs \n",
    "            \n",
    "        def make_indexes(self):\n",
    "            \"\"\"Creates the reverse table that maps states/observations names\n",
    "            to their index in the probabilities array\"\"\"\n",
    "            self.Y_index = {}\n",
    "            for i in range(self.N):\n",
    "                self.Y_index[self.omega_Y[i]] = i\n",
    "            self.X_index = {}\n",
    "            for i in range(self.M):\n",
    "                self.X_index[self.omega_X[i]] = i\n",
    "      \n",
    "        def get_observationIndices( self, observations ):\n",
    "            \"\"\"return observation indices, i.e \n",
    "            return [self.O_index[o] for o in observations]\n",
    "            and deals with OOVs\n",
    "            \"\"\"\n",
    "            indices = zeros( len(observations), int )\n",
    "            k = 0\n",
    "            for o in observations:\n",
    "                if o in self.X_index:\n",
    "                    indices[k] = self.X_index[o]\n",
    "                else:\n",
    "                    indices[k] = UNKid\n",
    "                k += 1\n",
    "            return indices\n",
    "\n",
    "    \n",
    "        def data2indices(self, sent): \n",
    "            \"\"\"From one tagged sentence of the brown corpus: \n",
    "            - extract the words and tags \n",
    "            - returns two list of indices, one for each\n",
    "            -> (wordids, tagids)\n",
    "            \"\"\"\n",
    "            wordids = list()\n",
    "            tagids  = list()\n",
    "            for couple in sent:\n",
    "                wrd = couple[0]\n",
    "                tag = couple[1]\n",
    "                if wrd in self.X_index:\n",
    "                    wordids.append(self.X_index[wrd])\n",
    "                else:\n",
    "                    wordids.append(UNKid)\n",
    "                tagids.append(self.Y_index[tag])\n",
    "            return wordids,tagids\n",
    "            \n",
    "        def observation_estimation(self, pair_counts):\n",
    "            \"\"\" Build the observation distribution: \n",
    "                observation_proba is the observation probablility matrix\n",
    "                    [b_ki],  b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "                \n",
    "                pair_counts is dictionary with \n",
    "                - key : a tuple (word,tag)\n",
    "                - value: the associated count\n",
    "                \n",
    "                We just need to fill the matrix and normalize the count in the right way: \n",
    "                - one column (i constant) is the distrib. of word given a tag\n",
    "                - just normalize the column, i.e sum over the row (axis=0)\n",
    "            \"\"\"\n",
    "            # fill with counts\n",
    "            for pair in pair_counts:\n",
    "                wrd=pair[0] # get word \n",
    "                tag=pair[1] # get tag  \n",
    "                cpt=pair_counts[pair] # get the count\n",
    "                # get word index (row), deal with OOV\n",
    "                k = 0 # for <unk>\n",
    "                if wrd in self.X_index: \n",
    "                    k=self.X_index[wrd]\n",
    "                # get tag  index (column)\n",
    "                i=self.Y_index[tag]\n",
    "                # fill the matrix\n",
    "                self.observation_proba[k,i]=cpt\n",
    "            # normalize\n",
    "            self.observation_proba=self.observation_proba+self.smoothing_obs\n",
    "            self.observation_proba=self.observation_proba/self.observation_proba.sum(axis=0).reshape(1,self.N)\n",
    "            \n",
    "        \n",
    "        def transition_estimation(self, trans_counts):\n",
    "            \"\"\" Build the transition distribution: \n",
    "                transition_proba is the transition matrix with : \n",
    "                [a_ij] a[i,j] = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "                \n",
    "                trans_counts is dictionary with \n",
    "                - key : a tuple (tag-1,tag)\n",
    "                - value: the associated count\n",
    "                \n",
    "                We just need to fill the matrix and normalize the count in the right way: \n",
    "                - one column (i constant) is the distrib. of tags for  given context tag-1\n",
    "                - just normalize the column, i.e sum over the row (axis=0)\n",
    "            \"\"\"\n",
    "            # fill with counts\n",
    "            for pair in trans_counts:\n",
    "                i=self.Y_index[pair[1]]\n",
    "                j=self.Y_index[pair[0]]\n",
    "                self.transition_proba[i,j]=trans_counts[pair]\n",
    "            # normalize\n",
    "            self.transition_proba=self.transition_proba/self.transition_proba.sum(axis=0).reshape(1,self.N)\n",
    "        \n",
    "        def init_estimation(self, init_counts):\n",
    "            \"\"\"Build the init. distribution\"\"\"\n",
    "            # fill with counts\n",
    "            for tag in init_counts:\n",
    "                i=self.Y_index[tag]\n",
    "                self.initial_state_proba[i]=init_counts[tag]\n",
    "            # normalize\n",
    "            self.initial_state_proba=self.initial_state_proba/sum(self.initial_state_proba)\n",
    "             \n",
    "        \n",
    "        def supervised_training(self, pair_counts, trans_counts,init_counts):\n",
    "            \"\"\" Train the HMM's parameters. This function wraps everything\"\"\"\n",
    "            self.observation_estimation(pair_counts)\n",
    "            self.transition_estimation(trans_counts)\n",
    "            self.init_estimation(init_counts)\n",
    "            \n",
    "                    \n",
    "        def viterbi(self, obsids):\n",
    "            \"\"\" Viterbi Algorithm : \n",
    "            Finding the most likely sequence of hidden states. \n",
    "            \"\"\"\n",
    "\n",
    "            T = len(obsids)\n",
    "\n",
    "            # Initialisation\n",
    "            delta = zeros(self.N, float)\n",
    "            tmp = zeros(self.N, float)\n",
    "            psi = zeros((T, self.N), int)\n",
    "\n",
    "            # Delta update\n",
    "            delta_t = zeros(self.N, float)\n",
    "\n",
    "            # Apply initial_state probabilities to the first frame\n",
    "            delta = self.observation_proba[obsids[0]] * self.initial_state_proba\n",
    "\n",
    "            # Recursion\n",
    "            for t in xrange(1, T):\n",
    "                Obs_t = obsids[t]\n",
    "                if t == 1:\n",
    "                    for i in range(self.N):\n",
    "                        for j in range(self.N):\n",
    "                            tmp[j] = delta[j] * self.transition_head_proba[i, j]\n",
    "                        psi[t, i] = tmp.argmax()\n",
    "                        delta_t[i] = tmp.max() * self.observation_proba[Obs_t, i]\n",
    "                else:\n",
    "                    for i in range(self.N):\n",
    "                        for j in range(self.N):\n",
    "                            tmp[j] = delta[j] * self.transition_proba[\n",
    "                                i, psi[t - 1, j] * self.N + j]\n",
    "                        psi[t, i] = tmp.argmax()\n",
    "                        delta_t[i] = tmp.max() * self.observation_proba[Obs_t, i]\n",
    "\n",
    "                delta, delta_t = delta_t, delta\n",
    "\n",
    "            # Reconstruction\n",
    "            i_star = [delta.argmax()]\n",
    "            for psi_t in psi[-1:0:-1]:\n",
    "                i_star.append(psi_t[i_star[-1]])\n",
    "            i_star.reverse()\n",
    "\n",
    "            return i_star\n",
    "        \n",
    "        \n",
    "        def accuracy(self, test):\n",
    "            \"\"\" Compute and print accuracy on test \"\"\"\n",
    "            nb_correct = 0\n",
    "            total = 0\n",
    "            after_hmm = 0\n",
    "            false = 0\n",
    "            corrected_false = 0\n",
    "            corrected_by_hmm = 0\n",
    "            m = 0\n",
    "\n",
    "            for word in test:\n",
    "                obsids, statids = hmm.data2indices(word)\n",
    "                best_sequence = hmm.viterbi(obsids)\n",
    "\n",
    "                # Correct caracters after viterbi\n",
    "                nb_correct += sum(np.array(best_sequence) == np.array(statids))\n",
    "\n",
    "                # Number of the typos after hmm\n",
    "                after_hmm += sum(np.array(best_sequence) != np.array(statids))\n",
    "\n",
    "                # Number of typos (before hmm2)\n",
    "                false += sum(np.array([-1 + x for x in obsids]) != np.array(statids))\n",
    "                if (np.array(best_sequence) != np.array(statids)).all():\n",
    "                    corrected_false += sum(\n",
    "                        np.array([-1 + x for x in obsids]) != np.array(best_sequence))\n",
    "\n",
    "                # Number of typos corrected by hmm2\n",
    "                if (np.array(best_sequence) == np.array(statids)).all():\n",
    "                    corrected_by_hmm += sum(\n",
    "                        np.array([-1 + x for x in obsids]) != np.array(best_sequence))\n",
    "\n",
    "                total += len(statids)\n",
    "\n",
    "            print(corrected_false)\n",
    "            print(corrected_by_hmm)\n",
    "            print(false)\n",
    "            print(after_hmm)\n",
    "\n",
    "            print(\"Accuracy : {}% \".format(nb_correct * 100.0 / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compter les mots et les tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_counts(corpus):\n",
    "    \"\"\" \n",
    "    Build different count tables to train a HMM. Each count table is a dictionnary. \n",
    "    Returns: \n",
    "    * c_words: word counts\n",
    "    * c_tags: tag counts\n",
    "    * c_pairs: count of pairs (word,tag)\n",
    "    * c_transitions: count of tag bigram \n",
    "    * c_inits: count of tag found in the first position\n",
    "    \"\"\"\n",
    "    c_words = dict()\n",
    "    c_tags = dict()\n",
    "    c_pairs= dict()\n",
    "    c_transitions = dict()\n",
    "    c_inits = dict()\n",
    "    for sent in corpus:\n",
    "        # we use i because of the transition counts\n",
    "        for i in range(len(sent)):\n",
    "            couple=sent[i]\n",
    "            wrd = couple[0]\n",
    "            tag = couple[1]\n",
    "            # word counts\n",
    "            if wrd in c_words:\n",
    "                c_words[wrd]=c_words[wrd]+1\n",
    "            else:\n",
    "                c_words[wrd]=1\n",
    "            # tag counts\n",
    "            if tag in c_tags:\n",
    "                c_tags[tag]=c_tags[tag]+1\n",
    "            else:\n",
    "                c_tags[tag]=1\n",
    "            # observation counts\n",
    "            if couple in c_pairs:\n",
    "                c_pairs[couple]=c_pairs[couple]+1\n",
    "            else:\n",
    "                c_pairs[couple]=1\n",
    "            # i >  0 -> transition counts\n",
    "            if i > 0:\n",
    "                trans = (sent[i-1][1],tag) # (tag at t-1 , tag at t)\n",
    "                if trans in c_transitions:\n",
    "                    c_transitions[trans]=c_transitions[trans]+1\n",
    "                else:\n",
    "                    c_transitions[trans]=1\n",
    "            # i == 0 -> counts for initial states\n",
    "            else:\n",
    "                if tag in c_inits:\n",
    "                    c_inits[tag]=c_inits[tag]+1\n",
    "                else:\n",
    "                    c_inits[tag]=1\n",
    "                    \n",
    "    return c_words,c_tags,c_pairs, c_transitions, c_inits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du vocabulaire (filtrage selon le nombre d'occurence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_vocab(c_words, threshold):\n",
    "    \"\"\" \n",
    "    return a vocabulary by thresholding word counts. \n",
    "    inputs: \n",
    "    * c_words : a dictionnary that maps word to its counts\n",
    "    * threshold: count must be >= to the threshold to be included\n",
    "    \n",
    "    returns: \n",
    "    * a word list\n",
    "    \"\"\"\n",
    "    voc = list()\n",
    "    voc.append(UNK)\n",
    "    for w in c_words:\n",
    "        if c_words[w] >= threshold:\n",
    "            voc.append(w)\n",
    "    return voc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path = 'typos-data/'\n",
    "\n",
    "# Données avec 10% de typos\n",
    "train10 = pickle.load(open(path+'train10.pkl', 'rb'))\n",
    "test10 = pickle.load(open(path+'test10.pkl', 'rb'))\n",
    "\n",
    "# Données avec 20% de typos\n",
    "train20 = pickle.load(open(path+'train20.pkl', 'rb'))\n",
    "test20 = pickle.load(open(path+'test20.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lettres  : 26\n",
      "Nombre de tags  : 26\n",
      "Nombre de paires: 127\n",
      "Nombre de trans : 403 / 144\n",
      "Nombre de init. : 25\n",
      "{'a': 10560, 'c': 4808, 'b': 2070, 'e': 18091, 'd': 4541, 'g': 2736, 'f': 3379, 'i': 10976, 'h': 6683, 'k': 590, 'j': 108, 'm': 3773, 'l': 6417, 'o': 11935, 'n': 9778, 'q': 150, 'p': 3217, 's': 9762, 'r': 8247, 'u': 3931, 't': 13877, 'w': 2229, 'v': 1927, 'y': 2985, 'x': 274, 'z': 124}\n",
      "Vocabulaire :27\n"
     ]
    }
   ],
   "source": [
    "train = train10\n",
    "\n",
    "cwords,ctags,cpairs,ctrans,cinits = make_counts(train)\n",
    "print \"Nombre de lettres  : \"+str(len(cwords))\n",
    "print \"Nombre de tags  : \"+str(len(ctags))\n",
    "print \"Nombre de paires: \"+str(len(cpairs))\n",
    "print \"Nombre de trans : \"+str(len(ctrans))+ \" / \"+ str(12*12)\n",
    "print \"Nombre de init. : \"+str(len(cinits))\n",
    "print ctags\n",
    "vocab = make_vocab(cwords,10)\n",
    "print \"Vocabulaire :\"+str(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "26 states\n",
      "27 observations\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(state_list=ctags.keys(), observation_list=vocab,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "26 states\n",
      "27 observations\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(state_list=ctags.keys(), observation_list=vocab,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None,\n",
    "                 smoothing_obs = 0.001)\n",
    "hmm.supervised_training(cpairs,ctrans,cinits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4270\n",
      "2\n",
      "745\n",
      "6754\n",
      "Accuracy : 7.73224043716% \n"
     ]
    }
   ],
   "source": [
    "hmm.accuracy(test10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
